{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e103e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b7c8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config\n",
    "LOAD_JSON = False\n",
    "LOAD_PICKLE = True\n",
    "UPDATE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a076bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_json():\n",
    "    base_dir = 'DATA/digikala_products/all_data/'\n",
    "    data = []\n",
    "    for i in tqdm.tqdm(os.listdir(base_dir)):\n",
    "        path = base_dir+i\n",
    "        data += json.load(open(path, encoding=\"utf8\"))\n",
    "    print(f'# of data loaded = {len(data)}')\n",
    "    \n",
    "    data_dict = {}\n",
    "    for dic in tqdm.tqdm(data):\n",
    "        if 'recommendations' not in dic['data'].keys():\n",
    "            k = dic['data']['product']['id']\n",
    "            data_dict[k] = dic\n",
    "    print(f'# of active data loaded ={len(data_dict.keys())}')\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def pickle_data(data):\n",
    "    path = 'DATA/pickled/'\n",
    "    vers = os.listdir(path)\n",
    "    last_ver = float(vers[-1].split('_')[-1].split('.')[0])\n",
    "    new_ver = last_ver + 0.1\n",
    "    \n",
    "    np.save(f'DATA/pickled/data_{new_ver}.npy', data)\n",
    "    print(f'pickled at DATA/pickled/data_{new_ver}.npy')\n",
    "    \n",
    "def load_pickle():\n",
    "    path = 'DATA/pickled/'\n",
    "    vers = os.listdir(path)\n",
    "    last_ver = vers[-1]\n",
    "    \n",
    "    return np.load(path+last_ver, allow_pickle=True).item()\n",
    "\n",
    "def get_request(i, use_recom=False):\n",
    "    #time.sleep(1)\n",
    "    ok = False\n",
    "    while ok == False:\n",
    "        ok = True\n",
    "        try: r = requests.get(f'https://api.digikala.com/v2/product/{str(i)}/')\n",
    "        except:\n",
    "            print('network interupted: 60s')\n",
    "            ok = False\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "            \n",
    "        c = r.json()\n",
    "        status = c['status']\n",
    "        if r.ok:\n",
    "            if 'recommendations' not in c['data'].keys():\n",
    "                return c\n",
    "#             else:\n",
    "#                 # might have bugs\n",
    "#                 if use_recom:\n",
    "#                     try: c = get_request(c['data']['recommendations']['related_products']['products'][0]['id'])\n",
    "#                     except:\n",
    "#                         print(f'no related products found for id:{i}')\n",
    "#                     return c\n",
    "#                 else:\n",
    "#                     return c\n",
    "\n",
    "        else:\n",
    "            print(f'err: {status} for id:{i} trying again!! in 60s')\n",
    "            time.sleep(60)\n",
    "            c = get_request(i)\n",
    "            \n",
    "def get_image(url):\n",
    "    ok = False\n",
    "    while ok == False:\n",
    "        ok = True\n",
    "        try: \n",
    "            r = requests.get(url)\n",
    "        except:\n",
    "            print('network interupted: 60s')\n",
    "            ok = False\n",
    "            time.sleep(60)\n",
    "            continue\n",
    "        if r.ok:\n",
    "            c = r.content\n",
    "        else:\n",
    "            print(f'err: bad status for id:{i} trying again!! in 60s')\n",
    "            time.sleep(60)\n",
    "            c = get_image(url)\n",
    "    return c\n",
    "            \n",
    "def clean_data(data):\n",
    "    cleaned = {}\n",
    "    for k in tqdm.tqdm(data.keys()):\n",
    "        cleaned[k] = {}\n",
    "        # id===================================\n",
    "        try:cleaned[k]['id'] = data[k]['data']['product']['id']\n",
    "        except:cleaned[k]['id'] = None\n",
    "        # title================================\n",
    "        try:cleaned[k]['title'] = data[k]['data']['product']['title_fa']\n",
    "        except:cleaned[k]['title'] = None\n",
    "        # category=============================\n",
    "        try:cleaned[k]['category'] = data[k]['data']['product']['category']['id']\n",
    "        except:cleaned[k]['category'] = None\n",
    "        # brand================================\n",
    "        try:cleaned[k]['brand'] = data[k]['data']['product']['brand']['id']\n",
    "        except:cleaned[k]['brand'] = None\n",
    "        try:cleaned[k]['is_premium'] = data[k]['data']['product']['brand']['is_premium']\n",
    "        except:cleaned[k]['is_premium'] = None\n",
    "        try:cleaned[k]['is_miscellaneous'] = data[k]['data']['product']['brand']['is_miscellaneous']\n",
    "        except:cleaned[k]['is_miscellaneous'] = None\n",
    "        # rating================================\n",
    "        try:cleaned[k]['r_rate'] = data[k]['data']['product']['rating']['rate']\n",
    "        except:cleaned[k]['r_rate'] = None\n",
    "        try:cleaned[k]['r_count'] = data[k]['data']['product']['rating']['count']\n",
    "        except:cleaned[k]['r_count'] = None\n",
    "        # properties============================\n",
    "        try:cleaned[k]['is_fake'] = data[k]['data']['product']['properties']['is_fake']\n",
    "        except:cleaned[k]['is_fake'] = None\n",
    "        try:cleaned[k]['is_jet'] = data[k]['data']['product']['properties']['is_jet_eligible']\n",
    "        except:cleaned[k]['is_jet'] = None\n",
    "        try:cleaned[k]['is_med'] = data[k]['data']['product']['properties']['is_medical_supplement']\n",
    "        except:cleaned[k]['is_med'] = None\n",
    "#         # product_badges========================\n",
    "#         try:\n",
    "#             badges = data[k]['data']['product']['product_badges']\n",
    "#             cleaned[k]['badges'] = ''\n",
    "#             for b in badges:\n",
    "#                 cleaned[k]['badges'] += str(b['id'])\n",
    "#                 cleaned[k]['badges'] += ','\n",
    "#         except: cleaned[k]['badges'] = None\n",
    "#         # colors================================\n",
    "#         try:\n",
    "#             badges = data[k]['data']['product']['colors']\n",
    "#             cleaned[k]['colors'] = ''\n",
    "#             for b in badges:\n",
    "#                 cleaned[k]['colors'] += str(b['id'])\n",
    "#                 cleaned[k]['colors'] += ','\n",
    "#         except: cleaned[k]['colors'] = None\n",
    "        # size_guide============================\n",
    "        try:cleaned[k]['has_size_guide'] = data[k]['data']['product']['size_guide'] != []\n",
    "        except:cleaned[k]['has_size_guide'] = None\n",
    "        # price=================================\n",
    "        try:cleaned[k]['price'] = data[k]['data']['intrack']['eventData']['unitPrice']\n",
    "        except:cleaned[k]['price'] = None\n",
    "        # image================================\n",
    "        try:\n",
    "            Image.open(f'DATA/Images/{k}.png')\n",
    "        except:\n",
    "            url = data[k]['data']['seo']['twitter_card']['image']\n",
    "            img = get_image(url)\n",
    "            with open(f'DATA/Images/{k}.png', 'wb') as handler:\n",
    "                handler.write(img)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if np.asarray(Image.open(f'DATA/Images/{k}.png')).sum() == 0:\n",
    "#             print(f'getting_image: {k}')\n",
    "#             try:\n",
    "#                 url = data[k]['data']['seo']['twitter_card']['image']\n",
    "#                 img = get_image(url)\n",
    "# #                 img.save(f'DATA/Images/missed/{k}.png')\n",
    "#                 with open(f'DATA/Images/missed/{k}.png', 'wb') as handler:\n",
    "#                         handler.write(img)\n",
    "#             except:\n",
    "#                 img = Image.new('RGB', (800, 800))\n",
    "#                 img.save(f'DATA/Images/missed/{k}.png')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39521b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_request(2110364)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6718c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_JSON:\n",
    "    data_dict = load_data_from_json()\n",
    "elif LOAD_PICKLE:\n",
    "    data_dict = load_pickle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87230d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('DATA/train.csv')\n",
    "df_nominated = pd.read_csv('DATA/nominated_p_ids.csv',index_col=0)\n",
    "df_test = pd.read_csv('DATA/test_ids.csv')\n",
    "unique_ids = pd.concat([df_train.source_product_id, df_train.rel_product_id, df_nominated.rel_product_id, df_test.source_product_id]).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89666b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if UPDATE:\n",
    "    for k in tqdm.tqdm(unique_ids.values):\n",
    "        if k not in data_dict.keys():\n",
    "            r = get_request(k)\n",
    "            if r is not None:\n",
    "                print(f'adding id:{k}')\n",
    "                data_dict[k] = r\n",
    "    pickle_data(data_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c4ce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70206"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89cee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|███████████▉                                                              | 11348/70206 [42:34<2:45:45,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████▏                                              | 24551/70206 [1:34:54<2:23:13,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████▎                                 | 37297/70206 [2:28:04<1:41:09,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|██████████████████████████████████████▎                                 | 37325/70206 [2:29:32<3:43:19,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████▌                    | 50308/70206 [3:22:27<1:10:20,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|██████████████████████████████████████████████████████▋                 | 53378/70206 [3:36:12<1:02:24,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n",
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████▊               | 55850/70206 [5:07:47<43:55,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n",
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████▊    | 66264/70206 [5:54:05<21:57,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network interupted: 60s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 70206/70206 [6:11:35<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = clean_data(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc57aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_l = []\n",
    "for k in cleaned_data.keys():\n",
    "    temp_l.append(cleaned_data[k].values())\n",
    "    \n",
    "df_id2feat = pd.DataFrame(temp_l,index=cleaned_data.keys(), columns=cleaned_data[350].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b78820",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id2feat.to_csv('DATA/id2feat3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56368bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
